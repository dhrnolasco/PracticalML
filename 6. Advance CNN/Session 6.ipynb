{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 6\n",
    "\n",
    "Here we will address some advance topics related to Convolutional Neural Networks. Also, we will explore the use of an algorithm named \"YOLO\" to detect and extract multiple objects from a single image.\n",
    "\n",
    "## Learning Objectives:\n",
    "* Use pre-existing trained models \n",
    "* Visualize what the network learned\n",
    "* Techniques to accelerate your model training\n",
    "    * Data Augmentation\n",
    "    * Existing pre-trained model\n",
    "* (Bonus) Use YOLO: You Only ~~Live~~ Look Once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1. Use Pre-Existing Trained Models](./Python/1.%20Convnets.ipynb)\n",
    "\n",
    "Keras provides several pre-trained models that can be used directly. These models are pre-trained on ImageNet dataset, and are quite large in size. The full list of models can be found on [Keras website](https://keras.io/applications/).\n",
    "\n",
    "Each model has different accuracy and performance, so depending on your application you can choose which model most benifitial.\n",
    "\n",
    "<img src=\"./images/1.convnet.PNG\" width=\"60%\">\n",
    "\n",
    "\n",
    "<br>Usually its fairly easy and straight forward to apply these models into your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2. Visualize what the Network Learned](./Python/2.%20Visualize%20CNN.ipynb)\n",
    "\n",
    "So after you have trained your model, is it possible to look into the model's \"brain\"? What are the layers activations, and how they react to the input? \n",
    "\n",
    "The layers for ConvNets are basically set of kernels or filters, so we can visualize the model using two ways:\n",
    "* By providing an input image, and calculate the activations for each of the filters for each layer, and then displaying them\n",
    "\n",
    "<img src=\"./images/2.input.PNG\" width=\"30%\">\n",
    "<img src=\"./images/2.activations.PNG\" width=\"100%\">\n",
    "\n",
    "\n",
    "* By looking at the kernels and visualize what patterns activate them. For this, a simple gradient descent algorithm is used.\n",
    "<img src=\"./images/2.kernels.PNG\" width=\"70%\">\n",
    "\n",
    "\n",
    "For more thourogh details about how visualization works, check this video by Matt Zeiler explaining it \n",
    "\n",
    "<br>[![Matt Zeiler](https://img.youtube.com/vi/ghEmQSxT6tw/0.jpg)](https://www.youtube.com/watch?v=ghEmQSxT6tw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3. Techniques to Accelerate Model Training (CNN)](./Python/3.%20Advance%20Training.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Data Augmentation to Generate new Images\n",
    "\n",
    "\n",
    "Data augmentation is a technique to expand the size of the dataset by applying simple image transformation on the input dataset. Here is an example of augmented image:\n",
    "\n",
    "<img src=\"./images/3.ImageAugmentation.PNG\" width=\"70%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pre-Trained Network\n",
    "\n",
    "Here, we can use an existing trained model from keras, and only include its convnet part without the Dense layers. The reason is Dense layers are the one in charge of defining the network's labels. Thus, we can later on add our own dense network on top of it, and train it as we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.applications.vgg16 import VGG16,preprocess_input,decode_predictions\n",
    "#First time calling this will download the entire model (~500MB)\n",
    "conv_net=VGG16(weights='imagenet',include_top=False,input_shape=(150,150,3))\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(conv_net, to_file='images/3.conv_net_loaded.png',show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The result of the model architecture looks like the following:\n",
    "<img src=\"./Images/3.conv_net_loaded.png\" width=\"30%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can notice the last layer is MaxPooling2D, not Dense layer. \n",
    "\n",
    "<br>Following code will add new layers on top of that model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models,layers\n",
    "\n",
    "model=models.Sequential()\n",
    "conv_net.trainable=False #<--- Freeze it, as we don't want to retrain the conv-net again\n",
    "model.add(conv_net)# <---- here we add the entire convnet model to create a cascaded models\n",
    "model.add(layers.Flatten())#Flatten the last layer, so we can use our own dense layers\n",
    "model.add(layers.Dense(256,activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(2,activation='softmax'))\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, to_file='images/3.conv_net_dense.png',show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The result of the model architecture looks like the following:\n",
    "<img src=\"./Images/3.conv_net_dense.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [(Bonus) YOLO](./Python/4.%20YOLO/YOLO%20Object%20Detection.ipynb)\n",
    "\n",
    "YOLO is based on Convolutional Neural Networks to extract features from an image, and predict the class of each region of the image, and then to segment the image based on the detected features and predictions.\n",
    "\n",
    "You can read more details about this technique from the original paper:\n",
    "<br>[YOLOv3: An Incremental Improvement by Joseph Redmon and Ali Farhadi](https://arxiv.org/abs/1804.02767)\n",
    "\n",
    "or from the website:\n",
    "<br>[https://pjreddie.com/darknet/yolo/](https://pjreddie.com/darknet/yolo/)\n",
    "\n",
    "<img src=\"./Images/4.YOLO.PNG\" width=\"90%\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PML",
   "language": "python",
   "name": "pml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
