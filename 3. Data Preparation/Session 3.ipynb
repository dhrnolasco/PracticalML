{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3\n",
    "\n",
    "This lecture covers the datasets and how to prepared them for training, evaluating, and testing.\n",
    "\n",
    "\n",
    "## Learning Objectives:\n",
    "* Building a dataset\n",
    "    * Download images from google service\n",
    "    * Preparing dataset folder structure\n",
    "    * Collect images from camera or by sketching them\n",
    "* Evaluate performance of training\n",
    "\n",
    "## Practical Examples:\n",
    "* Build a general image classifier\n",
    "* Build an audio recognition system\n",
    "\n",
    "## Required Libraries:\n",
    "__Scikit-learn__: Useful library for data processing. We use it here to split training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install scikit-learn -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Building Dataset\n",
    "\n",
    "Preparding a dataset for training can be troublesome and time consuming process. To accelerate this process, we can use online websites and search engines to build an initial dataset for training.\n",
    "\n",
    "For this session, we will use google images search engine to retrive images with specific keywords.\n",
    "<br>Google Images Downloader: [GoogleImageDownload.ipynb](./Python/GoogleImageDownload.ipynb)\n",
    "\n",
    "That notebook will help you at retriving images with selected keywords and amount. Running it will take time depending on the network connection and number of entries per category.\n",
    "\n",
    "Another way is to download an existing dataset online, most famous websites for datascience is [Kaggle](https://www.kaggle.com/). \n",
    "\n",
    "When preparding a dataset, its important to use a way to organize it. The simplist way is to use folder structure defining the labels and samples per label. For example:\n",
    "<br>__Shoes__\n",
    "<br>|-__children__\n",
    "<br>|----|- imgA.jpg\n",
    "<br>|----|- imgB.jpg\n",
    "<br>|----| ....\n",
    "<br>|----|- imgN.jpg\n",
    "<br>|-__men__\n",
    "<br>|----|- imgA.jpg\n",
    "<br>|----|- imgB.jpg\n",
    "<br>|----| ....\n",
    "<br>|----|- imgN.jpg\n",
    "<br>|-__women__\n",
    "<br>|----|- imgA.jpg\n",
    "<br>|----|- imgB.jpg\n",
    "<br>|----| ....\n",
    "<br>|----|- imgN.jpg\n",
    "\n",
    "\n",
    "When you have a complex type to train (for example, images and sensor readings combined), its best to have an excel like sheet that refers to the values of each sample. For example:\n",
    "\n",
    "|ID | Image|Color| Size | Style   |\n",
    "|---|---|---|---|---|\n",
    "| Sample 1 | ImgA.jpg | Red   | 27   | Westren |\n",
    "| Sample 2 | ImgB.jpg | Black | 29.5 | Asian   |\n",
    "| ...      | ...| ... | ...|\n",
    "\n",
    "\n",
    "\n",
    "Some helpful examples that use Processing to send images to python, and to help at building your dataset:\n",
    "* Python Notebook for building the dataset: [OSCImageDataset.ipynb](./Python/OSCImageDataset.ipynb)\n",
    "_<br>Make sure to specify the name of your dataset, and labels to be used_\n",
    "\n",
    "__Processingstreamers:__\n",
    "* Camera Images: [Camera_Streamer.pde](./Processing/Camera_Streamer/Camera_Streamer.pde)\n",
    "* Image Sketches: [Sketch_Streamer.ipynb](./Sketch_Streamer/Sketch_Streamer.pde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Evaluate performance of training\n",
    "\n",
    "When training the network for specific dataset, how can we tell if it can really generalize to unseen data? Solution is simple! We split the dataset to two main parts:\n",
    "* __Train & Validation dataset (75% of all data)__: This dataset is used for training purpose, the training part of it (usually 80%) is used internally by the network to tune the weights of the layers, and the validation set (20%) is used to observe its performance and tune network parameters (layers count, number of perceptrons, ...etc). We also observe both loss functions of the training and validation while the network is training.\n",
    "\n",
    "We can use this function from scikit-learn library to split the dataset (x_train,y_train) to two datasets: train and test (its called test, but its the validation dataset).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(x_train,y_train,test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Test dataset (25% of all data)__: This one is used *After* the model has been trained, and this will give us the overall performance of the network for unseen data, and how it will perform on reality. \n",
    "\n",
    "In the audio and image classifier examples, the validation is used in the fit function:\n",
    "<br>*model.fit(X_train,Y_train,__validation_data=(X_test,Y_test)__,epochs=100,batch_size=8)*\n",
    "\n",
    "You will notice that when training, validation accuracy is also reported:\n",
    "\n",
    "<img src=\"Images/training_metrics.png\" width=\"80%\">\n",
    "\n",
    "\n",
    "---\n",
    "### Visualize training performance using Tensorboard\n",
    "\n",
    "\n",
    "Tensorboard is a great tool to display the performance of training your model against your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#don't run this code here! Its just to show the newly added tensorboard\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from time import time\n",
    "\n",
    "## .... Model created, data is pre-processed and split to training (X_train, Y_train) and validation (X_test,Y_test)\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "model.fit(X_train,Y_train,validation_data=(X_test,Y_test),epochs=100,batch_size=8, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, in the command line (or terminal), access notebook location and run this command:\n",
    "__<br>tensorboard --logdir=logs/__\n",
    "\n",
    "<img src=\"Images/tensorboard_terminal.png\" width=\"50%\">\n",
    "\n",
    "Then, in the browser (chrome for example), access the following URL:\n",
    "\n",
    "[http://localhost:6006](http://localhost:6006)\n",
    "\n",
    "If the port (6006) in was different from the one in the picture, then change the port part in the URL to the new port. You can compare multiple training sessions with different hyber paramteters of the model (number of layers, number of perceptrons, ...etc) or with/without normalization. \n",
    "\n",
    "<br>This picture shows the results after tuning the network parameters to achieve best accuracy for both training and validation datasets:\n",
    "\n",
    "<img src=\"Images/tensorboard_metrics.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Practical Examples\n",
    "\n",
    "### General Image Classifier:\n",
    "\n",
    "<br>[ImageClassifier.ipynb](./Python/ImageClassifier.ipynb) allows you to train on a custom dataset of images\n",
    "\n",
    "\n",
    "### Audio Recognizer:\n",
    "\n",
    "<br>[AudioRecognizer.ipynb](./Python/AudioRecognizer.ipynb) allows you to train on a custom dataset of audio files. It shows the process how to preprocess audio files for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PML",
   "language": "python",
   "name": "pml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
